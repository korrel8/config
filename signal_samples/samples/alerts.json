{
  "labels": {
    "alertname": "PodSecurityViolation",
    "namespace": "openshift-kube-apiserver",
    "openshift_io_alert_source": "platform",
    "policy_level": "restricted",
    "prometheus": "openshift-monitoring/k8s",
    "severity": "info"
  },
  "annotations": {
    "description": "A workload (pod, deployment, deamonset, ...) was created somewhere in the cluster but it did not match the PodSecurity \"restricted\" profile defined by its namespace either via the cluster-wide configuration (which triggers on a \"restricted\" profile violations) or by the namespace local Pod Security labels. Refer to Kubernetes documentation on Pod Security Admission to learn more about these violations.",
    "summary": "One or more workloads users created in the cluster don't match their Pod Security profile"
  },
  "startsAt": "2022-10-17T20:35:12.63Z",
  "endsAt": "2022-10-20T15:12:12.63Z",
  "generatorURL": "https:///console-openshift-console.apps.snoflake.my.test/monitoring/graph?g0.expr=sum+by%28policy_level%29+%28increase%28pod_security_evaluations_total%7Bdecision%3D%22deny%22%2Cmode%3D%22audit%22%2Cresource%3D%22pod%22%7D%5B1d%5D%29%29+%3E+0&g0.tab=1",
  "status": {
    "state": "active",
    "silencedBy": null,
    "inhibitedBy": null
  },
  "receivers": [
    "Default"
  ],
  "fingerprint": "1abee08c98d0de7f"
}
{
  "labels": {
    "alertname": "HighOverallControlPlaneMemory",
    "openshift_io_alert_source": "platform",
    "prometheus": "openshift-monitoring/k8s",
    "severity": "warning"
  },
  "annotations": {
    "description": "Given three control plane nodes, the overall memory utilization may only be about 2/3 of all available capacity. This is because if a single control plane node fails, the kube-apiserver and etcd my be slow to respond. To fix this, increase memory of the control plane nodes.",
    "summary": "Memory utilization across all control plane nodes is high, and could impact responsiveness and stability."
  },
  "startsAt": "2022-10-20T12:20:39.607Z",
  "endsAt": "2022-10-20T15:12:39.607Z",
  "generatorURL": "https:///console-openshift-console.apps.snoflake.my.test/monitoring/graph?g0.expr=%281+-+sum%28node_memory_MemFree_bytes+%2B+node_memory_Buffers_bytes+%2B+node_memory_Cached_bytes+and+on%28instance%29+label_replace%28kube_node_role%7Brole%3D%22master%22%7D%2C+%22instance%22%2C+%22%241%22%2C+%22node%22%2C+%22%28.%2B%29%22%29%29+%2F+sum%28node_memory_MemTotal_bytes+and+on%28instance%29+label_replace%28kube_node_role%7Brole%3D%22master%22%7D%2C+%22instance%22%2C+%22%241%22%2C+%22node%22%2C+%22%28.%2B%29%22%29%29%29+%2A+100+%3E+60&g0.tab=1",
  "status": {
    "state": "active",
    "silencedBy": null,
    "inhibitedBy": null
  },
  "receivers": [
    "Default"
  ],
  "fingerprint": "1d913cd6f088bf8f"
}
{
  "labels": {
    "alertname": "KubeDeploymentReplicasMismatch",
    "container": "kube-rbac-proxy-main",
    "deployment": "minio",
    "endpoint": "https-main",
    "job": "kube-state-metrics",
    "namespace": "openshift-logging",
    "openshift_io_alert_source": "platform",
    "prometheus": "openshift-monitoring/k8s",
    "service": "kube-state-metrics",
    "severity": "warning"
  },
  "annotations": {
    "description": "Deployment openshift-logging/minio has not matched the expected number of replicas for longer than 15 minutes. This indicates that cluster infrastructure is unable to start or restart the necessary components. This most often occurs when one or more nodes are down or partioned from the cluster, or a fault occurs on the node that prevents the workload from starting. In rare cases this may indicate a new version of a cluster component cannot start due to a bug or configuration error. Assess the pods for this deployment to verify they are running on healthy nodes and then contact support.",
    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeDeploymentReplicasMismatch.md",
    "summary": "Deployment has not matched the expected number of replicas"
  },
  "startsAt": "2022-10-19T20:54:04.19Z",
  "endsAt": "2022-10-20T15:13:04.19Z",
  "generatorURL": "https:///console-openshift-console.apps.snoflake.my.test/monitoring/graph?g0.expr=%28%28%28kube_deployment_spec_replicas%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7Cdefault%29%22%7D+%3E+kube_deployment_status_replicas_available%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7Cdefault%29%22%7D%29+and+%28changes%28kube_deployment_status_replicas_updated%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7Cdefault%29%22%7D%5B5m%5D%29+%3D%3D+0%29%29+%2A+on%28%29+group_left%28%29+cluster%3Acontrol_plane%3Aall_nodes_ready%29+%3E+0&g0.tab=1",
  "status": {
    "state": "active",
    "silencedBy": null,
    "inhibitedBy": null
  },
  "receivers": [
    "Default"
  ],
  "fingerprint": "67c35393de41d01f"
}
{
  "labels": {
    "alertname": "Watchdog",
    "namespace": "openshift-monitoring",
    "openshift_io_alert_source": "platform",
    "prometheus": "openshift-monitoring/k8s",
    "severity": "none"
  },
  "annotations": {
    "description": "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n",
    "summary": "An alert that should always be firing to certify that Alertmanager is working properly."
  },
  "startsAt": "2022-10-17T20:30:53.481Z",
  "endsAt": "2022-10-20T15:12:23.481Z",
  "generatorURL": "https:///console-openshift-console.apps.snoflake.my.test/monitoring/graph?g0.expr=vector%281%29&g0.tab=1",
  "status": {
    "state": "active",
    "silencedBy": null,
    "inhibitedBy": null
  },
  "receivers": [
    "Watchdog"
  ],
  "fingerprint": "6934731368443c07"
}
{
  "labels": {
    "alertname": "AlertmanagerReceiversNotConfigured",
    "namespace": "openshift-monitoring",
    "openshift_io_alert_source": "platform",
    "prometheus": "openshift-monitoring/k8s",
    "severity": "warning"
  },
  "annotations": {
    "description": "Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager.",
    "summary": "Receivers (notification integrations) are not configured on Alertmanager"
  },
  "startsAt": "2022-10-17T20:32:04.19Z",
  "endsAt": "2022-10-20T15:13:34.19Z",
  "generatorURL": "https:///console-openshift-console.apps.snoflake.my.test/monitoring/graph?g0.expr=cluster%3Aalertmanager_integrations%3Amax+%3D%3D+0&g0.tab=1",
  "status": {
    "state": "active",
    "silencedBy": null,
    "inhibitedBy": null
  },
  "receivers": [
    "Default"
  ],
  "fingerprint": "72bc0ebbd3167d00"
}
{
  "labels": {
    "alertname": "KubePodNotReady",
    "namespace": "openshift-logging",
    "openshift_io_alert_source": "platform",
    "pod": "minio-b7c668ffb-jjhxp",
    "prometheus": "openshift-monitoring/k8s",
    "severity": "warning"
  },
  "annotations": {
    "description": "Pod openshift-logging/minio-b7c668ffb-jjhxp has been in a non-ready state for longer than 15 minutes.",
    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePodNotReady.md",
    "summary": "Pod has been in a non-ready state for more than 15 minutes."
  },
  "startsAt": "2022-10-19T20:54:08.915Z",
  "endsAt": "2022-10-20T15:13:08.915Z",
  "generatorURL": "https:///console-openshift-console.apps.snoflake.my.test/monitoring/graph?g0.expr=sum+by%28namespace%2C+pod%2C+cluster%29+%28max+by%28namespace%2C+pod%2C+cluster%29+%28kube_pod_status_phase%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7Cdefault%29%22%2Cphase%3D~%22Pending%7CUnknown%22%7D%29+%2A+on%28namespace%2C+pod%2C+cluster%29+group_left%28owner_kind%29+topk+by%28namespace%2C+pod%2C+cluster%29+%281%2C+max+by%28namespace%2C+pod%2C+owner_kind%2C+cluster%29+%28kube_pod_owner%7Bowner_kind%21%3D%22Job%22%7D%29%29%29+%3E+0&g0.tab=1",
  "status": {
    "state": "active",
    "silencedBy": null,
    "inhibitedBy": null
  },
  "receivers": [
    "Default"
  ],
  "fingerprint": "d67b8097d015b6d2"
}
{
  "labels": {
    "alertname": "ClusterNotUpgradeable",
    "condition": "Upgradeable",
    "endpoint": "metrics",
    "name": "version",
    "openshift_io_alert_source": "platform",
    "prometheus": "openshift-monitoring/k8s",
    "severity": "info"
  },
  "annotations": {
    "description": "In most cases, you will still be able to apply patch releases. Reason IncompatibleOperatorsInstalled. For more information refer to 'oc adm upgrade' or https://console-openshift-console.apps.snoflake.my.test/settings/cluster/.",
    "summary": "One or more cluster operators have been blocking minor version cluster upgrades for at least an hour."
  },
  "startsAt": "2022-10-18T20:37:56.403Z",
  "endsAt": "2022-10-20T15:13:26.403Z",
  "generatorURL": "https:///console-openshift-console.apps.snoflake.my.test/monitoring/graph?g0.expr=max+by%28name%2C+condition%2C+endpoint%29+%28cluster_operator_conditions%7Bcondition%3D%22Upgradeable%22%2Cendpoint%3D%22metrics%22%2Cname%3D%22version%22%7D+%3D%3D+0%29&g0.tab=1",
  "status": {
    "state": "active",
    "silencedBy": null,
    "inhibitedBy": null
  },
  "receivers": [
    "Default"
  ],
  "fingerprint": "edf14056dd177353"
}
